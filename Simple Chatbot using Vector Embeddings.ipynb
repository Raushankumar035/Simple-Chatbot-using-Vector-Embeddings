{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"s9rVNjpHaRlU"},"outputs":[],"source":["!pip -q install chromadb sentence-transformers transformers accelerate einops --upgrade"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QDOp1xCRacSe"},"outputs":[],"source":["import os\n","import time\n","import uuid\n","import json\n","from dataclasses import dataclass, asdict\n","from typing import List, Dict, Optional, Tuple\n","\n","import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n","\n","import chromadb\n","from chromadb.config import Settings\n","\n","from sentence_transformers import SentenceTransformer\n","\n","import textwrap"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":111,"status":"ok","timestamp":1761985028827,"user":{"displayName":"Raushan kumar","userId":"16750871066435246886"},"user_tz":-330},"id":"JQ51MgKzatFM","outputId":"719e3f56-7124-4db3-98d2-f26429c64922"},"outputs":[{"name":"stdout","output_type":"stream","text":["Persist dir: /content/memory_db\n"]}],"source":["CHAT_MODEL = \"Qwen/Qwen2.5-0.5B-Instruct\"\n","EMBED_MODEL = \"all-MiniLM-L6-v2\"\n","PERSIST_DIR = \"/content/memory_db\"\n","\n","os.makedirs(PERSIST_DIR, exist_ok=True)\n","print(\"Persist dir:\", PERSIST_DIR)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d2kp8Y0fauxc"},"outputs":[],"source":["class SBERTEmbeddings:\n","    def __init__(self, model_name: str = EMBED_MODEL):\n","        self.model = SentenceTransformer(model_name)\n","\n","    def embed_texts(self, texts: List[str]) -> List[List[float]]:\n","        vecs = self.model.encode(\n","            texts, convert_to_numpy=True, normalize_embeddings=True)\n","        return [v.tolist() for v in vecs]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IIGWtX-ia7sC"},"outputs":[],"source":["client = chromadb.PersistentClient(\n","    path=PERSIST_DIR, settings=Settings(anonymized_telemetry=False))\n","collection = client.get_or_create_collection(\n","    name=\"chat_memory\", metadata={\"hnsw:space\": \"cosine\"})"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9065,"status":"ok","timestamp":1761985043012,"user":{"displayName":"Raushan kumar","userId":"16750871066435246886"},"user_tz":-330},"id":"sHOCirm1bLa_","outputId":"6a5938f5-5909-49f4-f6c6-f3d2a9feba5c"},"outputs":[{"name":"stderr","output_type":"stream","text":["Device set to use cpu\n"]}],"source":["tokenizer = AutoTokenizer.from_pretrained(CHAT_MODEL)\n","model = AutoModelForCausalLM.from_pretrained(\n","    CHAT_MODEL,\n","    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n","    device_map=\"auto\",\n",")\n","textgen = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vXWC0FAobMDo"},"outputs":[],"source":["def run_chat(messages: List[Dict[str, str]], max_new_tokens: int = 256, temperature: float = 0.2) -> str:\n","    prompt = tokenizer.apply_chat_template(\n","        messages, tokenize=False, add_generation_prompt=True)\n","    out = textgen(prompt, max_new_tokens=max_new_tokens,\n","                  do_sample=True, temperature=temperature, top_p=0.9, top_k=50)\n","    gen = out[0][\"generated_text\"][len(prompt):]\n","    return gen.strip()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qr5mwdDAbWDr"},"outputs":[],"source":["@dataclass\n","class MemoryItem:\n","    id: str\n","    role: str\n","    content: str\n","    user_id: str\n","    conversation_id: str\n","    created_at: float"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5E8WSw8WbZQQ"},"outputs":[],"source":["def add_to_memory(items: List[MemoryItem]):\n","    texts = [i.content for i in items]\n","    embeddings = embedder.embed_texts(texts)\n","    collection.add(\n","        ids=[i.id for i in items],\n","        documents=texts,\n","        metadatas=[asdict(i) for i in items],\n","        embeddings=embeddings,\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bK3iGOpfbcMr"},"outputs":[],"source":["SYS_PROMPT = (\n","    \"You are a concise research assistant. \"\n","    \"When helpful, cite retrieved notes inline as [memory n]. If memory seems irrelevant, say so briefly before answering.\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u7j-7Fs_bfjh"},"outputs":[],"source":["def build_context(retrieved, char_limit: int = 1600):\n","    chunks, used = [], 0\n","    for i, (doc, meta, dist) in enumerate(retrieved, start=1):\n","        s = f\"[memory {i}] role={meta.get('role')} dist={dist:.3f} at {int(meta.get('created_at',0))}\\n{doc.strip()}\\n\"\n","        if used + len(s) > char_limit:\n","            break\n","        chunks.append(s)\n","        used += len(s)\n","    return \"\\n\".join(chunks) if chunks else \"(no highly relevant memory found)\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OOXsVnh3bitg"},"outputs":[],"source":["def answer(user_msg: str, top_k: int = 6) -> str:\n","    global CONV_ID\n","    retrieved = retrieve(user_msg, top_k=top_k, where={\"user_id\": USER_ID})\n","    context = build_context(retrieved)\n","    messages = [\n","        {\"role\": \"system\", \"content\": SYS_PROMPT},\n","        {\"role\": \"user\", \"content\": f\"Use the MEMORY below only if helpful. Then answer the QUESTION.\\n\\nMEMORY:\\n{context}\\n\\nQUESTION: {user_msg}\"},\n","    ]\n","    reply = run_chat(messages)\n","\n","    add_to_memory([\n","        MemoryItem(str(uuid.uuid4()), \"user\", user_msg,\n","                   USER_ID, CONV_ID, now_ts()),\n","        MemoryItem(str(uuid.uuid4()), \"assistant\",\n","                   reply, USER_ID, CONV_ID, now_ts()),\n","    ])\n","    return reply"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1504,"status":"ok","timestamp":1761985063490,"user":{"displayName":"Raushan kumar","userId":"16750871066435246886"},"user_tz":-330},"id":"OYcQliYGdzif","outputId":"9db70f5c-9b14-459b-fb39-7396968ec82b"},"outputs":[{"name":"stdout","output_type":"stream","text":["User ID: user_6cd26615\n","Conversation ID: conv_7fab7d19\n"]}],"source":["# ==== Global Variables & Utility Function ====\n","\n","def now_ts():\n","    \"\"\"Return current timestamp as float\"\"\"\n","    return time.time()\n","\n","# Give unique IDs for this user and conversation\n","USER_ID = \"user_\" + str(uuid.uuid4())[:8]\n","CONV_ID = \"conv_\" + str(uuid.uuid4())[:8]\n","\n","# Initialize the embedder (if not already done)\n","embedder = SBERTEmbeddings(EMBED_MODEL)\n","\n","print(\"User ID:\", USER_ID)\n","print(\"Conversation ID:\", CONV_ID)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VJ2oHJcTbl8w"},"outputs":[],"source":["seed_items = [\n","    MemoryItem(str(uuid.uuid4()), \"user\", \"My name is Geek.\",\n","               USER_ID, CONV_ID, now_ts()),\n","    MemoryItem(str(uuid.uuid4()), \"assistant\",\n","               \"Nice to meet you, Geek!\", USER_ID, CONV_ID, now_ts()),\n","    MemoryItem(str(uuid.uuid4()), \"user\",\n","               \"I prefer examples in Python, not JavaScript.\", USER_ID, CONV_ID, now_ts()),\n","]\n","add_to_memory(seed_items)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bjfiVW7Zf5pP"},"outputs":[],"source":["# =========================\n","# ðŸ” Memory Functions Setup\n","# =========================\n","\n","def add_to_memory(items: List[MemoryItem]):\n","    texts = [i.content for i in items]\n","    embeddings = embedder.embed_texts(texts)\n","    collection.add(\n","        ids=[i.id for i in items],\n","        documents=texts,\n","        metadatas=[asdict(i) for i in items],\n","        embeddings=embeddings,\n","    )\n","\n","def retrieve(query: str, top_k: int = 6, where: Optional[Dict] = None):\n","    q_emb = embedder.embed_texts([query])[0]\n","    res = collection.query(\n","        query_embeddings=[q_emb],\n","        n_results=top_k,\n","        where=where,\n","        include=[\"documents\", \"metadatas\", \"distances\"],\n","    )\n","    docs = res.get(\"documents\", [[]])[0]\n","    metas = res.get(\"metadatas\", [[]])[0]\n","    dists = res.get(\"distances\", [[]])[0]\n","    return list(zip(docs, metas, dists))\n","\n","def build_context(retrieved, char_limit: int = 1600):\n","    chunks, used = [], 0\n","    for i, (doc, meta, dist) in enumerate(retrieved, start=1):\n","        s = f\"[memory {i}] role={meta.get('role')} dist={dist:.3f} at {int(meta.get('created_at',0))}\\n{doc.strip()}\\n\"\n","        if used + len(s) > char_limit:\n","            break\n","        chunks.append(s)\n","        used += len(s)\n","    return \"\\n\".join(chunks) if chunks else \"(no highly relevant memory found)\"\n","\n","def answer(user_msg: str, top_k: int = 6) -> str:\n","    global CONV_ID\n","    retrieved = retrieve(user_msg, top_k=top_k, where={\"user_id\": USER_ID})\n","    context = build_context(retrieved)\n","    messages = [\n","        {\"role\": \"system\", \"content\": SYS_PROMPT},\n","        {\"role\": \"user\", \"content\": f\"Use the MEMORY below only if helpful. Then answer the QUESTION.\\n\\nMEMORY:\\n{context}\\n\\nQUESTION: {user_msg}\"},\n","    ]\n","    reply = run_chat(messages)\n","\n","    add_to_memory([\n","        MemoryItem(str(uuid.uuid4()), \"user\", user_msg,\n","                   USER_ID, CONV_ID, now_ts()),\n","        MemoryItem(str(uuid.uuid4()), \"assistant\",\n","                   reply, USER_ID, CONV_ID, now_ts()),\n","    ])\n","    return reply\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"LC5j-ewnbrwF","outputId":"f831496f-1f7d-4933-dad8-a4d90d237200"},"outputs":[{"name":"stdout","output_type":"stream","text":["Assistant: AI stands for Artificial Intelligence, which refers to the simulation of human intelligence in machines that are programmed to think and learn like humans. AI involves developing algorithms and models that can perform tasks typically associated with humans, such as visual perception, speech recognition, decision-making, and problem-solving. It aims to create intelligent systems capable of learning from data, understanding natural language, and performing tasks autonomously or under certain conditions.\n","Assistant: The term \"array\" refers to a collection of elements that store related data values. Arrays are fundamental data structures used in programming languages to organize collections of items (such as numbers, strings, or other objects) into a sequence of elements. They allow developers to easily access and manipulate these elements by specifying their position within the array. This makes arrays versatile tools for various applications, including data storage, processing, and communication in computer science and software development.\n"]}],"source":["while True:\n","    user_msg = input(\"You: \")\n","    if user_msg.lower().strip() in [\"exit\", \"quit\", \"stop\"]:\n","        print(\"Exiting chat...\")\n","        break\n","    reply = answer(user_msg)\n","    print(\"Assistant:\", reply)"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPmvolJrKmy95OGpQ7XXrH3"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}